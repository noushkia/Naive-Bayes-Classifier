{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3dc7ae",
   "metadata": {},
   "source": [
    "\n",
    "<h1> Computer Assignment 3 Report </h1>\n",
    "<h2> Artificial Intelligence Course - University of Tehran - Fall 1400 </h2>\n",
    "<h2> Naive Bayes Classifier </h2>\n",
    "<h3> Name: Kianoush Arshi <br>\n",
    " Student ID: 810198438 </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4513b8",
   "metadata": {},
   "source": [
    "In this assignment, we'll be using a naive bayes classifier to classify a dataset of advertisements. There are six different classes:<br>\n",
    "<li>Vehicles</li>\n",
    "<li>Electronic devices</li>\n",
    "<li>Businesses</li>\n",
    "<li>For the home</li>\n",
    "<li>Personal</li>\n",
    "<li>Leisure & Hobbies</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6478d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41daf0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>بلبل خرمایی</td>\n",
       "      <td>سه عدد بلبل خرمایی سه ماهه.از وقتی جوجه بودن خ...</td>\n",
       "      <td>leisure-hobbies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>عینک اسکی در حد</td>\n",
       "      <td>عینک اسکی دبل لنز مارک يو وكس  در حد نو اصلی م...</td>\n",
       "      <td>leisure-hobbies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>تکیه سر تویوتا پرادو</td>\n",
       "      <td>پارچه ای سالم و تمیز.</td>\n",
       "      <td>vehicles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>مجسمه کریستال24%</td>\n",
       "      <td>مجسمه دکوری کریستال بالرین Rcr24%</td>\n",
       "      <td>for-the-home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>کیف و ساک</td>\n",
       "      <td>هر 2 کاملا تمیز هستند</td>\n",
       "      <td>personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10195</th>\n",
       "      <td>ان هاش 85</td>\n",
       "      <td>نیمه دوم همه چی به شرط در حد خشک 260تا کار</td>\n",
       "      <td>vehicles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10196</th>\n",
       "      <td>405 دوگانه کارخانه. تمیز</td>\n",
       "      <td>فابریک 4 حلقه لاستیک 205 نو بیمه یکسال تخفیف ب...</td>\n",
       "      <td>vehicles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10197</th>\n",
       "      <td>بخاری گازی دودکش دار پلار</td>\n",
       "      <td>بخاری نو و بسیار تمیز هستش\\nبا مشتری واقعی کنا...</td>\n",
       "      <td>for-the-home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10198</th>\n",
       "      <td>نر کله برنجی چتری</td>\n",
       "      <td>سلام به دلیل کمبود جا واسباب کشی به کمترین قیم...</td>\n",
       "      <td>leisure-hobbies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10199</th>\n",
       "      <td>پراید111سفید</td>\n",
       "      <td>پراید111se\\nسفید.مدل93.درب جلو سمت شاگرد استوک...</td>\n",
       "      <td>vehicles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10200 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           title  \\\n",
       "0                    بلبل خرمایی   \n",
       "1                عینک اسکی در حد   \n",
       "2           تکیه سر تویوتا پرادو   \n",
       "3               مجسمه کریستال24%   \n",
       "4                      کیف و ساک   \n",
       "...                          ...   \n",
       "10195                  ان هاش 85   \n",
       "10196   405 دوگانه کارخانه. تمیز   \n",
       "10197  بخاری گازی دودکش دار پلار   \n",
       "10198          نر کله برنجی چتری   \n",
       "10199               پراید111سفید   \n",
       "\n",
       "                                             description       categories  \n",
       "0      سه عدد بلبل خرمایی سه ماهه.از وقتی جوجه بودن خ...  leisure-hobbies  \n",
       "1      عینک اسکی دبل لنز مارک يو وكس  در حد نو اصلی م...  leisure-hobbies  \n",
       "2                                  پارچه ای سالم و تمیز.         vehicles  \n",
       "3                      مجسمه دکوری کریستال بالرین Rcr24%     for-the-home  \n",
       "4                                  هر 2 کاملا تمیز هستند         personal  \n",
       "...                                                  ...              ...  \n",
       "10195         نیمه دوم همه چی به شرط در حد خشک 260تا کار         vehicles  \n",
       "10196  فابریک 4 حلقه لاستیک 205 نو بیمه یکسال تخفیف ب...         vehicles  \n",
       "10197  بخاری نو و بسیار تمیز هستش\\nبا مشتری واقعی کنا...     for-the-home  \n",
       "10198  سلام به دلیل کمبود جا واسباب کشی به کمترین قیم...  leisure-hobbies  \n",
       "10199  پراید111se\\nسفید.مدل93.درب جلو سمت شاگرد استوک...         vehicles  \n",
       "\n",
       "[10200 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Data/divar_train.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad338918",
   "metadata": {},
   "source": [
    "First, let's check the count of advertisements in each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d0e8d7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "leisure-hobbies       1700\n",
       "vehicles              1700\n",
       "for-the-home          1700\n",
       "personal              1700\n",
       "electronic-devices    1700\n",
       "businesses            1700\n",
       "Name: categories, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['categories'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d634351",
   "metadata": {},
   "source": [
    "As mentioned in the project description, there are equal number of categories so ne resampling is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f459c25a",
   "metadata": {},
   "source": [
    "<h2>Phase 1: Pre-processing the Data</h2>\n",
    "<p>In this phase, the dataset is edited so that it'll be able to be used efficiently and correctly in the future. The changes made to the dataset include:</p>\n",
    "<li>Stemming</li>\n",
    "<li>Lemmatizing</li>\n",
    "<li>Tokenizing</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e37995",
   "metadata": {},
   "source": [
    "<h3>Stemming and lemmatization</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98fdbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "\n",
    "stemmer = Stemmer()\n",
    "lemmatizer = Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f015d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بلبل خرما\n",
      "خوابیدند\n",
      "خوابید\n",
      "خواب\n",
      "بخواب\n",
      "خوابید\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(df['title'][0]))\n",
    "print(stemmer.stem('خوابیدند'))\n",
    "print(stemmer.stem('خوابید'))\n",
    "print(stemmer.stem('خواب'))\n",
    "print(stemmer.stem('بخوابی'))\n",
    "print(stemmer.stem('خوابیدم'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e50762",
   "metadata": {},
   "source": [
    "Stemmer reduces the words to the root word. This isn't very much useful for us since it removes some parts of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3eabfea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "بلبل خرمایی\n",
      "خوابید#خواب\n",
      "خوابید#خواب\n",
      "خواب\n",
      "خوابید#خواب\n",
      "خوابید#خواب\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(df['title'][0]))\n",
    "print(lemmatizer.lemmatize('خوابیدند'))\n",
    "print(lemmatizer.lemmatize('خوابید'))\n",
    "print(lemmatizer.lemmatize('خواب'))\n",
    "print(lemmatizer.lemmatize('بخوابی'))\n",
    "print(lemmatizer.lemmatize('خوابیدم'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cf12bb",
   "metadata": {},
   "source": [
    "Lemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meanings to one word. This is made possible by extracting principal parts of the verb. (bon e mazi # bon e mozare)\n",
    "Lemmatization is the preferred method over Stemming.<br>\n",
    "\n",
    "But for making use of lemmatization, we first need to tokenize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c5c390a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['بلبل', 'خرمایی']\n",
      "['سه', 'عدد', 'بلبل', 'خرمایی', 'سه', 'ماهه', '.', 'از', 'وقتی', 'جوجه', 'بودن', 'خودم', 'بزرگشون', 'کردم', 'اما', 'دستی', 'نیستن', 'واسه', 'همین', 'قیمت', 'پایین', 'دادم', '.', 'هر', 'سه', 'با', 'هم', '100', 'تومان', 'مقطوع', 'مقطوع']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(df['title'][0]))\n",
    "print(word_tokenize(df['description'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db92f338",
   "metadata": {},
   "source": [
    "The word_tokenize() function will breaks the sentence into it's words.<br>\n",
    "This will prove useful in the future since we'll be using bag of words model in solving the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf15bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "خرمایی\n",
      "خرما\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(df['title'][0])[1])\n",
    "print(lemmatizer.lemmatize(word_tokenize(df['title'][0])[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7581ca0",
   "metadata": {},
   "source": [
    "We need to save stop words so that they are ignored. Also, some characters need to e removed and the data needs to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "565e7fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['…',\n",
       " 'آاو و و و',\n",
       " 'آخ',\n",
       " 'آخر',\n",
       " 'آخرها',\n",
       " 'آخه',\n",
       " 'آدمهاست',\n",
       " 'آرام',\n",
       " 'آرام آرام',\n",
       " 'آره',\n",
       " 'آزادانه',\n",
       " 'آسان',\n",
       " 'آسيب پذيرند',\n",
       " 'آشكارا',\n",
       " 'آشنايند',\n",
       " 'آمرانه',\n",
       " 'آن',\n",
       " 'آن گاه',\n",
       " 'آن ها',\n",
       " 'آنان',\n",
       " 'آناني',\n",
       " 'آنجا',\n",
       " 'آنچنان',\n",
       " 'آنچنان كه',\n",
       " 'آنچه',\n",
       " 'آنرا',\n",
       " 'آنقدر',\n",
       " 'آنگاه',\n",
       " 'آنها',\n",
       " 'آنهاست']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stop_words.txt', encoding=\"utf8\") as stop_words:\n",
    "    ignore = [line.rstrip() for line in stop_words]\n",
    "\n",
    "ignore = [x.strip() for x in ignore]\n",
    "ignore[20:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d34bba6",
   "metadata": {},
   "source": [
    "<h2>Phase 1: Problem Solving Process</h2>\n",
    "<p>In this phase, we aim for solving the problem using Naive Bayes. As mentioned previously, bag of words model will be used for solving the problem. The feature used for classifying the advertisements is the number of words of each category used in the advertisement. The base formula used for classifying advertisements is as follows:</p><br>\n",
    "\n",
    "$$P(c|x)=\\frac{P(x|c)P(c)}{P(x)}$$\n",
    "\n",
    "$x$: The word(s) detected<br>\n",
    "$c$: Advertisement class<br>\n",
    "$P(c|x)$: Probability of the current class being $c$ knowing that the word $x$ has appeared in the title and/or description. (Posterior)<br>\n",
    "$P(x|c)$: Probability of seeing word $x$ in a class description of type $c$ (Likelihood)<br>\n",
    "$P(c)$: Probability of seeing a book with genre $c$. This is equal for all genres since they all have occured the same number of times in the dataset. (Class Prior Probability)<br>\n",
    "$P(x)$: Probability of seeing word $x$ in the context(Predictor Prior Probability (Evidence))<br>\n",
    "Note that x can be viewed as multiple words which then we will have the fllwoing formula:<br>\n",
    "$$P(c|X)=P(x_1|c)P(x_2|c)...P(x_n|c)P(c)$$\n",
    "\n",
    "The process of solving this classifying problem is listeb below:<br>\n",
    "<li> Tokenize the words (this can be unigram, bigram or ngram)\n",
    "<li> Classify the tokenized words and calculate the given probabilities of the above formula (all except P(c|x) which will be tested on test dataset).\n",
    "<li> Test the classifier.\n",
    "<li> Calculate the accuracy.\n",
    "<li> Repeat until we get an adequate accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ec318",
   "metadata": {},
   "source": [
    "Tokenizing the words:<br>\n",
    "First, we'll tokenize the title column and use that as our feature only.<br>\n",
    "We'll repeat this for description only and both title and description as features and choose the feature with best accuracy.<br>\n",
    "This part is related with Grouping Operations technique [1].<br>\n",
    "\n",
    "\n",
    "Bigrams and N-grams:<br>\n",
    "This part relates to Feature Splitting technique of feature engineering. To know more about this technique, visit source [1].<br>\n",
    "Note that using unigrams might increase inaccuracy of the model, for example check out the following snetences:<br>\n",
    "I left my phone in the room.<br>\n",
    "I'm left alone.<br>\n",
    "Left here has two meanings and we need to know more than one word in order to figure out what left means.<br>\n",
    "Persian example:<br>\n",
    "<br>شیر امید را خورد\n",
    "<br>امید شیر را خورد\n",
    "<br>\n",
    "This example is extremly difficult! For this one, not only we need to check all the words, but also we need to check their order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3a6409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c53d204",
   "metadata": {},
   "source": [
    "References:<br>\n",
    "[1] https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18d91d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
